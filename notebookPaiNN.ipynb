{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02456 Molecular Property Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic example of how to train the PaiNN model to predict the QM9 property\n",
    "\"internal energy at 0K\". This property (and the majority of the other QM9\n",
    "properties) is computed as a sum of atomic contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "from tqdm import trange\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import seed_everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QM9 Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.loader import DataLoader\n",
    "from typing import Optional, List, Union, Tuple\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "\n",
    "\n",
    "class GetTarget(BaseTransform):\n",
    "    def __init__(self, target: Optional[int] = None) -> None:\n",
    "        self.target = [target]\n",
    "\n",
    "\n",
    "    def forward(self, data: Data) -> Data:\n",
    "        if self.target is not None:\n",
    "            data.y = data.y[:, self.target]\n",
    "        return data\n",
    "\n",
    "\n",
    "class QM9DataModule(pl.LightningDataModule):\n",
    "\n",
    "    target_types = ['atomwise' for _ in range(19)]\n",
    "    target_types[0] = 'dipole_moment'\n",
    "    target_types[5] = 'electronic_spatial_extent'\n",
    "\n",
    "    # Specify unit conversions (eV to meV).\n",
    "    unit_conversion = {\n",
    "        i: (lambda t: 1000*t) if i not in [0, 1, 5, 11, 16, 17, 18]\n",
    "        else (lambda t: t)\n",
    "        for i in range(19)\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target: int = 7,\n",
    "        data_dir: str = 'data/',\n",
    "        batch_size_train: int = 100,\n",
    "        batch_size_inference: int = 1000,\n",
    "        num_workers: int = 0,\n",
    "        splits: Union[List[int], List[float]] = [110000, 10000, 10831],\n",
    "        seed: int = 0,\n",
    "        subset_size: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.target = target\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size_train = batch_size_train\n",
    "        self.batch_size_inference = batch_size_inference\n",
    "        self.num_workers = num_workers\n",
    "        self.splits = splits\n",
    "        self.seed = seed\n",
    "        self.subset_size = subset_size\n",
    "\n",
    "        self.data_train = None\n",
    "        self.data_val = None\n",
    "        self.data_test = None\n",
    "\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # Download data\n",
    "        QM9(root=self.data_dir)\n",
    "\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        dataset = QM9(root=self.data_dir, transform=GetTarget(self.target))\n",
    "\n",
    "        # Shuffle dataset\n",
    "        rng = np.random.default_rng(seed=self.seed)\n",
    "        dataset = dataset[rng.permutation(len(dataset))]\n",
    "\n",
    "        # Subset dataset\n",
    "        if self.subset_size is not None:\n",
    "            dataset = dataset[:self.subset_size]\n",
    "        \n",
    "        # Split dataset\n",
    "        if all([type(split) == int for split in self.splits]):\n",
    "            split_sizes = self.splits\n",
    "        elif all([type(split) == float for split in self.splits]):\n",
    "            split_sizes = [int(len(dataset) * prop) for prop in self.splits]\n",
    "\n",
    "        split_idx = np.cumsum(split_sizes)\n",
    "        self.data_train = dataset[:split_idx[0]]\n",
    "        self.data_val = dataset[split_idx[0]:split_idx[1]]\n",
    "        self.data_test = dataset[split_idx[1]:]\n",
    "\n",
    "\n",
    "    def get_target_stats(\n",
    "        self,\n",
    "        remove_atom_refs: bool = True,\n",
    "        divide_by_atoms: bool = True\n",
    "    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n",
    "        atom_refs = self.data_train.atomref(self.target)\n",
    "\n",
    "        ys = list()\n",
    "        for batch in self.train_dataloader(shuffle=False):\n",
    "            y = batch.y.clone()\n",
    "            if remove_atom_refs and atom_refs is not None:\n",
    "                y.index_add_(\n",
    "                    dim=0, index=batch.batch, source=-atom_refs[batch.z]\n",
    "                )\n",
    "            if divide_by_atoms:\n",
    "                _, num_atoms  = torch.unique(batch.batch, return_counts=True)\n",
    "                y = y / num_atoms.unsqueeze(-1)\n",
    "            ys.append(y)\n",
    "\n",
    "        y = torch.cat(ys, dim=0)\n",
    "        return y.mean(), y.std(), atom_refs\n",
    "\n",
    "\n",
    "    def train_dataloader(self, shuffle: bool = True) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.data_train,\n",
    "            batch_size=self.batch_size_train,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=shuffle,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.data_val,\n",
    "            batch_size=self.batch_size_inference,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.data_test,\n",
    "            batch_size=self.batch_size_inference,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class AtomwisePostProcessing(nn.Module):\n",
    "    \"\"\"\n",
    "    Post-processing for (QM9) properties that are predicted as sums of atomic\n",
    "    contributions.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_outputs: int,\n",
    "        mean: torch.FloatTensor,\n",
    "        std: torch.FloatTensor,\n",
    "        atom_refs: torch.FloatTensor,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_outputs: Integer with the number of model outputs. In most\n",
    "                cases 1.\n",
    "            mean: torch.FloatTensor with mean value to shift atomwise\n",
    "                contributions by.\n",
    "            std: torch.FloatTensor with standard deviation to scale atomwise\n",
    "                contributions by.\n",
    "            atom_refs: torch.FloatTensor of size [num_atom_types, 1] with\n",
    "                atomic reference values.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "        self.register_buffer('scale', std)\n",
    "        self.register_buffer('shift', mean)\n",
    "        self.atom_refs = nn.Embedding.from_pretrained(atom_refs, freeze=True)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        atomic_contributions: torch.FloatTensor,\n",
    "        atoms: torch.LongTensor,\n",
    "        graph_indexes: torch.LongTensor,\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Atomwise post-processing operations and atomic sum.\n",
    "\n",
    "        Args:\n",
    "            atomic_contributions: torch.FloatTensor of size [num_nodes,\n",
    "                num_outputs] with each node's contribution to the overall graph\n",
    "                prediction, i.e., each atom's contribution to the overall\n",
    "                molecular property prediction.\n",
    "            atoms: torch.LongTensor of size [num_nodes] with atom type of each\n",
    "                node in the graph.\n",
    "            graph_indexes: torch.LongTensor of size [num_nodes] with the graph \n",
    "                index each node belongs to.\n",
    "\n",
    "        Returns:\n",
    "            A torch.FLoatTensor of size [num_graphs, num_outputs] with\n",
    "            predictions for each graph (molecule).\n",
    "        \"\"\"\n",
    "        num_graphs = torch.unique(graph_indexes).shape[0]\n",
    "\n",
    "        atomic_contributions = atomic_contributions*self.scale + self.shift\n",
    "        atomic_contributions = atomic_contributions + self.atom_refs(atoms)\n",
    "\n",
    "        # Sum contributions for each graph\n",
    "        output_per_graph = torch.zeros(\n",
    "            (num_graphs, self.num_outputs),\n",
    "            device=atomic_contributions.device,\n",
    "        )\n",
    "        output_per_graph.index_add_(\n",
    "            dim=0,\n",
    "            index=graph_indexes,\n",
    "            source=atomic_contributions,\n",
    "        )\n",
    "\n",
    "        return output_per_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PaiNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1. Compute Scala Messages\n",
    "\n",
    "\\begin{align*}\n",
    "    m_{ij} \\ \\text{or} \\ h^n  & = \\phi_m (x_i, \\ x_j, \\ || \\vec{d}_{ij} || ) = \\mathsf{MLP}( [ x_i, x_j, || \\vec{d}_{ij} || ]) \\\\\n",
    "           \\Rightarrow M_i & = \\sum_{j \\in \\mathcal{N}(i) } m_{ij} x_j \\cdot \\vec{d}_{ij} \\ \\text{ aggregation }\\\\\n",
    "           \\Rightarrow x' & =  \\phi_m (x_i, M_i ) \\ \\text{ update }  \\\\  \n",
    "           & = x_i + M_i \n",
    "\\end{align*}\n",
    "\n",
    "* e.g. \n",
    "\n",
    "\\begin{align*}\n",
    "    m_{A} & = \\phi (x_A, \\ x_B, || \\vec{d}_{AB} || ) \\\\\n",
    "           & = \\mathsf{MLP}([0.5, 1.2, 0.8, 0.9, \\sqrt{2} ]) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "##### Note. Displacement magnitude\n",
    "\n",
    "\\begin{align*}\n",
    "    || \\vec{d}_{AB} || & = \\vec{r}_{A} - \\vec{r}_{B} \\\\\n",
    "    & = \\sqrt{ (-1)^2 + (1)^2 + (0)^2 } || \\\\\n",
    "    & = \\sqrt{2}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "We'll start with the following setting for the MLP, 2 laye network, input size 5, hidden size 4 and ouput size 2.\n",
    "\n",
    "1.3 Linear layer(Linear compbination) \n",
    "\n",
    "* e.g., \n",
    "\\begin{align*}\n",
    "    h^n = w_n m_i + b_n\n",
    "\\end{align*}\n",
    "\n",
    "1.4. Initialize weights and biases, typically they are initalized radomly.\n",
    "\n",
    "e.g.,\n",
    "\n",
    "\\begin{align*}\n",
    "    h^1 = [0.996,0.802,âˆ’0.168,0.009]\n",
    "\\end{align*}\n",
    "\n",
    "1.5. Apply activation function SiLU\n",
    "\n",
    "    SiLU(h^1) = ?\n",
    "\n",
    "1.6 Apply SiLU(h^1) to next connected layer(s)\n",
    "e.g.\n",
    "\\begin{align*}\n",
    "    m_{AB} \\text{ or } (h^2) = w_2 \\text{}(h^1) + b_2\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "2. Compute Vectorial Messages\n",
    "\n",
    "Vectorial messages are just matrix version of the function above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Message Block:\n",
    "Features-wise continuous-filterlter convolutions:\n",
    "\\begin{align*}\n",
    "    \\Delta {\\mathrm{s}_i}^{m} = & ( \\phi_s ( \\mathrm{s} ) * \\mathcal{W}_s )_i \\\\\n",
    "        = & \\sum_j \\phi_s ( \\mathrm{s}_j ) \\circ \\mathcal{W}_s ( || \\vec{r}_{ij} || )\n",
    "\\end{align*}\n",
    "\n",
    "The rotationally-invariant filter $\\mathcal{W}_s$ are linear combinations of radial basis function :\n",
    "\\begin{align*}\n",
    "    \\mathcal{W}_s ( || \\vec{r}_{ij} || ) = \\text{sin} (\\frac{n\\pi}{r_{cut}} || \\vec{r}_{ij}||) /  || \\vec{r}_{ij}||\n",
    "\\end{align*}\n",
    "\n",
    "cutoff function:\n",
    "\\begin{align}\n",
    "f_{cos}( || \\vec{r}_{ij} || ) & =\n",
    "    \\begin{cases}\n",
    "      0.5 \\cdot \\left( \\text{cos} \\left( \\frac { \\pi \\vec{r}_{ij} }{ r_{ \\text{cut} } }  \\right) \\right) & \\text{if} \\ || \\vec{r}_{ij} || \\leq r_{ \\text{cut} } \\\\\n",
    "      \\\\\n",
    "      0  & \\text{otherwise}\n",
    "    \\end{cases}    \\\\\n",
    "\\end{align}\n",
    "\n",
    "Continuous-filter convolutions for the residual of the equivariant message function\n",
    "\n",
    "\\begin{align*}\n",
    "    \\Delta \\vec{ \\mathrm{v}_i }^{m} = & \\sum_j \\vec{ \\mathrm{v}_j } \\circ \\phi_{vv} ( \\text{s}_j ) \\circ \\mathcal{ W }_{vv} ( || \\vec{r}_{ij} || ) \\\\\n",
    "        + & \\sum_j \\phi_{vs} ( \\text{s}_j ) \\circ \\mathcal{W}'_{vs} ( || \\vec{r}_{ij} || ) \\frac{ \\vec{r}_{ij} }{ || \\vec{r}_{ij} || } \n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Update Block:\n",
    "Atomwise udate accross features after the features-wise message passing, the residual of the scalar update function:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\Delta \\mathrm{s}_i^{u} = & \\mathrm{a}_{ss} ( \\mathrm{s}_i, || \\mathrm{V} \\mathrm{ \\vec{v} }_i || ) \\\\\n",
    "    + & \\mathrm{a}_{sv} ( \\mathrm{s}_i, || \\mathrm{V} \\mathrm{ \\vec{v} }_i || ) \\langle \\mathrm{U} \\mathrm{ \\vec{v}_i }, \\mathrm{V} \\mathrm{ \\vec{v}_i } \\rangle\n",
    "\\end{align*}\n",
    "\n",
    "Equivariant features\n",
    "\n",
    "\\begin{align*}\n",
    "    \\Delta \\mathrm{ \\vec{v} }_i^{u} = & \\mathrm{a}_{vv} ( \\mathrm{s}_i, || \\mathrm{V} \\mathrm{ \\vec{v} }_i || ) \\mathrm{U} \\mathrm{ \\vec{v} }_i\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.nn import radius_graph\n",
    "# data_module = QM9DataModule(target=7)\n",
    "# data_module.prepare_data()\n",
    "# data_module.setup()\n",
    "\n",
    "# train_loader = data_module.train_dataloader()\n",
    "# for batch in train_loader:\n",
    "#     print(batch)\n",
    "#     break\n",
    "\n",
    "# ############################## compute neighbour\n",
    "# edgeij = radius_graph(batch.pos, r=5.0, batch=batch.batch)\n",
    "# print(f\"edgeij {edgeij[0]} edgeij {edgeij[1]}\")\n",
    "# ### source is other node and target is current node\n",
    "# eij = radius_graph(batch.pos, r=5.0, batch=batch.batch,flow=\"source_to_target\")\n",
    "# print(f\"eij {eij[0]} eij {eij[1]}\")\n",
    "# eji = radius_graph(batch.pos, r=5.0, batch=batch.batch,flow=\"target_to_source\")\n",
    "# print(f\"eji {eji[0]} eji {eji[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.nn import radius_graph\n",
    "# import torch.nn as nn\n",
    "# from torch.nn import Linear, SiLU\n",
    "# from torch_scatter import scatter_sum\n",
    "# data_module = QM9DataModule(target=7)\n",
    "# data_module.prepare_data()\n",
    "# data_module.setup()\n",
    "\n",
    "# train_loader = data_module.train_dataloader()\n",
    "# for batch in train_loader:\n",
    "#     print(batch)\n",
    "#     break\n",
    "\n",
    "# ############################## compute neighbour\n",
    "# eij = radius_graph(batch.pos, r=5.0, batch=batch.batch,flow=\"source_to_target\")\n",
    "# #eij = radius_graph(batch.pos, r=5.0, batch=batch.batch,flow=\"target_to_source\")\n",
    "# #print(f\"neighbour {eij.shape}\")\n",
    "# #print(f\"neighbour {eij[0][0]}\")\n",
    "# #print(f\"neighbour {eij[0].shape}\")\n",
    "# #print(f\"neighbour {eij[1].shape}\")\n",
    "\n",
    "# ### vector distance \n",
    "# rij_vec = batch.pos[eij[0]] - batch.pos[eij[1]]\n",
    "\n",
    "# ### Norm\n",
    "# #rij_norm = torch.norm(batch.pos[eij[0]] - batch.pos[eij[1]], dim=-1, keepdim=True)\n",
    "# rij_norm = torch.norm(rij_vec, dim=-1)\n",
    "\n",
    "# ### normalization\n",
    "# rij_hat =  rij_vec / (rij_norm.unsqueeze(-1) + 1e-8)\n",
    "\n",
    "# def fCut(rij_norm, r_cut):\n",
    "#     f_cut = 0.5 * (torch.cos(torch.pi * rij_norm / r_cut) + 1)\n",
    "#     #print(f_cut)\n",
    "#     f_cut[rij_norm > r_cut] = 0  # Set values beyond cutoff to zero\n",
    "#     return f_cut\n",
    "\n",
    "# ### rbf \n",
    "# def fRBF(rij_norm, r_cut, n_rbf=20):\n",
    "#     t_rbf = torch.arange(1, n_rbf + 1, device=rij_norm.device).float()\n",
    "#     # Calculate RBF values\n",
    "#     rij_norm = rij_norm.unsqueeze(-1)  # Shape: [N, 1]\n",
    "    \n",
    "#     RBF = torch.sin(t_rbf * torch.pi * rij_norm / r_cut) / (rij_norm + 1e-8)\n",
    "#     # Mask for values beyond the cutoff\n",
    "#     # mask = (rij_norm <= r_cut).unsqueeze(-1)  # Shape: [N, 1]\n",
    "#     # Ws = Ws * mask.float()\n",
    "#     # Ws = Ws * fCut(rij_norm, r_cut)\n",
    "#     return RBF\n",
    "\n",
    "# RBF = fRBF(rij_norm, 5.0, 20)\n",
    "# print(f\"RBF: {RBF.shape}\")\n",
    "# ### Linear layer\n",
    "# RBF_Linear = Linear(20,384)\n",
    "# T_RBF = RBF_Linear(RBF)\n",
    "\n",
    "# Ws = T_RBF * fCut(rij_norm,5.0).unsqueeze(-1) \n",
    "\n",
    "# # print(f\"Ws.shape: {Ws.shape}\")\n",
    "\n",
    "\n",
    "# ### embeddings \n",
    "# S_embeddings = nn.Embedding(100, 128)\n",
    "# si= S_embeddings(batch['z'])\n",
    "# #print(si.shape, si[eij[1]].shape,si[eij[0]].shape)\n",
    "# vi = torch.zeros_like(si).unsqueeze(-1).repeat(1, 1, 3)\n",
    "\n",
    "# ##################################Message block\n",
    "# ### linear layers\n",
    "# S_Linear = nn.Sequential(\n",
    "#     Linear(in_features=128,\n",
    "#         out_features=128,\n",
    "#     ),\n",
    "#     SiLU(),\n",
    "#     Linear(in_features=128,\n",
    "#         out_features=384,\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# #sj = si[eij[1]]\n",
    "# sj = si[eij[0]]\n",
    "# print(f\"sj {sj.shape}\")\n",
    "# phi = S_Linear(sj)\n",
    "# # print(f\"phi linear shape {phi.shape}\")\n",
    "# # print(f\"phi linear {phi}\")\n",
    "\n",
    "# #vj = torch.zeros_like(si[eij[1]]).unsqueeze(-1)\n",
    "# #vj = vi[eij[1]]\n",
    "# vj = vi[eij[0]]\n",
    "# ### hadarmad product\n",
    "# phiW = phi * Ws\n",
    "# print(f\"vi {vi.shape} vj {vj.shape}\")\n",
    "# ### split\n",
    "# #Split_Linear = Linear(in_features=384, out_features=128, bias=False)\n",
    "\n",
    "# #Split = Split_Linear(phiW)\n",
    "# SPLIT1 = phiW[:,0:128]\n",
    "# SPLIT2 = phiW[:,128:256]\n",
    "# SPLIT3 = phiW[:,256:]\n",
    "# # print(f\"split1 {SPLIT1.shape}\")\n",
    "# # print(f\"split2 {SPLIT2.shape}\")\n",
    "# # print(f\"split3 {SPLIT3.shape}\")\n",
    "\n",
    "# ###########Second term\n",
    "# phiWvs = SPLIT3.unsqueeze(-1) * rij_hat.unsqueeze(1)\n",
    "# print(SPLIT3.unsqueeze(-1).shape, rij_hat.shape, rij_hat.unsqueeze(1).shape)\n",
    "# #################First term\n",
    "# phiWvv = vj * SPLIT1.unsqueeze(-1).repeat(1, 1, 3)\n",
    "\n",
    "# #d_vim = scatter_sum((phiWvv + phiWvs), eij[0], dim=0)\n",
    "# d_vim = scatter_sum((phiWvv + phiWvs), eij[1], dim=0)\n",
    "# print(f\"delta v shape{d_vim.shape}\")\n",
    "\n",
    "# #d_vim = scatter_sum((phiWvv + phiWvs), eij[0], dim=0)\n",
    "# d_sim = scatter_sum(SPLIT2, eij[1], dim=0)\n",
    "# print(f\"delta s shape{d_sim.shape}\")\n",
    "\n",
    "# #vi += d_vim\n",
    "# si += d_sim\n",
    "# print(f\"sim {si.shape}\")\n",
    "# vi += d_vim\n",
    "\n",
    "\n",
    "# #print(f\"vi before update {vi.shape}\")\n",
    "# #print(f\"si {si.shape} si[eij[0]] {si[eij[0]].shape} si[eij[1]] {si[eij[1]].shape }\")\n",
    "# #sj = si[eij[1]]\n",
    "# #vj = vi[eij[1]]\n",
    "# print(f\"vi {vi.shape}\")\n",
    "# Luu = nn.Sequential( nn.Linear(3, 3, bias=False) )\n",
    "# Luv = nn.Sequential( nn.Linear(3, 3, bias=False) )\n",
    "# # Uvj = Luu(vj)  # Learnable weights for U\n",
    "# # Vvj = Luv(vj)  # Learnable weights for V\n",
    "# Uv = Luu(vi)  \n",
    "# Vv = Luv(vi) \n",
    "# print(f\"Uv {Uv.shape}, Vv {Vv.shape}\")\n",
    "\n",
    "# #cnn\n",
    "# # Cu = nn.Sequential(\n",
    "# #     nn.Conv1d(in_channels=3, out_channels=1, kernel_size=1, stride=1),\n",
    "# # )\n",
    "\n",
    "# # # Reshape for Conv1d compatibility\n",
    "# # V = Cu(vj.permute(0, 2, 1)).squeeze(1)  # Shape: [M, 1, 128] to Shape: [M, 128]\n",
    "# # print(V)\n",
    "# # V_norm = torch.norm(V,dim=-1)\n",
    "\n",
    "# #V_norm = torch.norm(Vvj,dim=-1)\n",
    "# V_norm = torch.norm(Vv,dim=-1)\n",
    "# # print(f\"V_norm.shape {V_norm.shape}\")\n",
    "\n",
    "# #STACK = torch.hstack([V_norm, sj])\n",
    "# STACK = torch.hstack([V_norm, si])\n",
    "# #print(f\"STACK.shape {STACK.shape}\")\n",
    "\n",
    "# #SP = torch.sum(Uvj * Vvj, dim=-1) \n",
    "# SP = torch.sum(Uv * Vv, dim=-1) \n",
    "# print(f\"SP {SP.shape}\")\n",
    "\n",
    "# Lus = nn.Sequential(\n",
    "#     Linear(in_features=256, out_features=128, bias=False),\n",
    "#     SiLU(),\n",
    "#     Linear(in_features=128, out_features=384, bias=False),\n",
    "#     #Linear(in_features=384, out_features=128, bias=False),\n",
    "# )\n",
    "\n",
    "# SPLITu = Lus(STACK)\n",
    "# SPLITu1 = SPLITu[:, 0:128]\n",
    "# SPLITu2 = SPLITu[:, 128:256]\n",
    "# SPLITu3 = SPLITu[:, 256:]\n",
    "\n",
    "# #d_viu = scatter_sum((Uvj * SPLIT.unsqueeze(-1).repeat(1, 1, 3)), eij[0], dim=0)\n",
    "# d_viu = Uv * SPLITu1.unsqueeze(-1).repeat(1, 1, 3)\n",
    "# #print(f\"SPLIT[eij[0]].unsqueeze(-1).repeat(1, 1, 3) {SPLIT[eij[0]].unsqueeze(-1).repeat(1, 1, 3).shape}\")\n",
    "# #d_siu = scatter_sum(( SP * SPLIT[eij[0]] + SPLIT[eij[0]]), eij[0], dim=0)\n",
    "# d_siu = SP * SPLITu2 + SPLITu3\n",
    "\n",
    "# #print(f\"d_siu {d_siu.shape}\")\n",
    "# vi += d_viu\n",
    "# si += d_siu\n",
    "\n",
    "# Lr = nn.Sequential(\n",
    "#     Linear(in_features=128, out_features=64, bias=False),\n",
    "#     SiLU(),\n",
    "#     Linear(in_features=64, out_features=1, bias=False),\n",
    "# )\n",
    "# readout = Lr(si)\n",
    "# print(readout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.nn import radius_graph\n",
    "# import torch.nn as nn\n",
    "# from torch.nn import Linear, SiLU\n",
    "# from torch_scatter import scatter_sum\n",
    "# data_module = QM9DataModule(target=7)\n",
    "# data_module.prepare_data()\n",
    "# data_module.setup()\n",
    "\n",
    "# train_loader = data_module.train_dataloader()\n",
    "# for batch in train_loader:\n",
    "#     print(batch)\n",
    "#     break\n",
    "\n",
    "# ############################## compute neighbour\n",
    "# eij = radius_graph(batch.pos, r=5.0, batch=batch.batch,flow=\"source_to_target\")\n",
    "# #eij = radius_graph(batch.pos, r=5.0, batch=batch.batch,flow=\"target_to_source\")\n",
    "# #print(f\"neighbour {eij.shape}\")\n",
    "# #print(f\"neighbour {eij[0][0]}\")\n",
    "# #print(f\"neighbour {eij[0].shape}\")\n",
    "# #print(f\"neighbour {eij[1].shape}\")\n",
    "\n",
    "# ### vector distance \n",
    "# rij_vec = batch.pos[eij[0]] - batch.pos[eij[1]]\n",
    "\n",
    "# ### Norm\n",
    "# #rij_norm = torch.norm(batch.pos[eij[0]] - batch.pos[eij[1]], dim=-1, keepdim=True)\n",
    "# rij_norm = torch.norm(rij_vec, dim=-1)\n",
    "\n",
    "# ### normalization\n",
    "# rij_hat =  rij_vec / (rij_norm.unsqueeze(-1) + 1e-8)\n",
    "\n",
    "# def fCut(rij_norm, r_cut):\n",
    "#     f_cut = 0.5 * (torch.cos(torch.pi * rij_norm / r_cut) + 1)\n",
    "#     #print(f_cut)\n",
    "#     f_cut[rij_norm > r_cut] = 0  # Set values beyond cutoff to zero\n",
    "#     return f_cut\n",
    "\n",
    "# ### rbf \n",
    "# def fRBF(rij_norm, r_cut, n_rbf=20):\n",
    "#     t_rbf = torch.arange(1, n_rbf + 1, device=rij_norm.device).float()\n",
    "#     # Calculate RBF values\n",
    "#     rij_norm = rij_norm.unsqueeze(-1)  # Shape: [N, 1]\n",
    "    \n",
    "#     RBF = torch.sin(t_rbf * torch.pi * rij_norm / r_cut) / (rij_norm + 1e-8)\n",
    "#     # Mask for values beyond the cutoff\n",
    "#     # mask = (rij_norm <= r_cut).unsqueeze(-1)  # Shape: [N, 1]\n",
    "#     # Ws = Ws * mask.float()\n",
    "#     # Ws = Ws * fCut(rij_norm, r_cut)\n",
    "#     return RBF\n",
    "\n",
    "# RBF = fRBF(rij_norm, 5.0, 20)\n",
    "# print(f\"RBF: {RBF.shape}\")\n",
    "# ### Linear layer\n",
    "# RBF_Linear = Linear(20,384)\n",
    "# T_RBF = RBF_Linear(RBF)\n",
    "\n",
    "# Ws = T_RBF * fCut(rij_norm,5.0).unsqueeze(-1) \n",
    "\n",
    "# # print(f\"Ws.shape: {Ws.shape}\")\n",
    "\n",
    "\n",
    "# ### embeddings \n",
    "# S_embeddings = nn.Embedding(100, 128)\n",
    "# si= S_embeddings(batch['z'])\n",
    "# #print(si.shape, si[eij[1]].shape,si[eij[0]].shape)\n",
    "# vi = torch.zeros_like(si).unsqueeze(-1).repeat(1, 1, 3)\n",
    "\n",
    "# ##################################Message block\n",
    "# ### linear layers\n",
    "# S_Linear = nn.Sequential(\n",
    "#     Linear(in_features=128,\n",
    "#         out_features=128,\n",
    "#     ),\n",
    "#     SiLU(),\n",
    "#     Linear(in_features=128,\n",
    "#         out_features=384,\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# #sj = si[eij[1]]\n",
    "# sj = si[eij[0]]\n",
    "# print(f\"sj {sj.shape}\")\n",
    "# phi = S_Linear(sj)\n",
    "# # print(f\"phi linear shape {phi.shape}\")\n",
    "# # print(f\"phi linear {phi}\")\n",
    "\n",
    "# #vj = torch.zeros_like(si[eij[1]]).unsqueeze(-1)\n",
    "# #vj = vi[eij[1]]\n",
    "# vj = vi[eij[0]]\n",
    "# ### hadarmad product\n",
    "# phiW = phi * Ws\n",
    "# print(f\"vi {vi.shape} vj {vj.shape}\")\n",
    "# ### split\n",
    "# #Split_Linear = Linear(in_features=384, out_features=128, bias=False)\n",
    "\n",
    "# #Split = Split_Linear(phiW)\n",
    "# SPLIT1 = phiW[:,0:128]\n",
    "# SPLIT2 = phiW[:,128:256]\n",
    "# SPLIT3 = phiW[:,256:]\n",
    "# # print(f\"split1 {SPLIT1.shape}\")\n",
    "# # print(f\"split2 {SPLIT2.shape}\")\n",
    "# # print(f\"split3 {SPLIT3.shape}\")\n",
    "\n",
    "# ###########Second term\n",
    "# phiWvs = SPLIT3.unsqueeze(-1) * rij_hat.unsqueeze(1)\n",
    "# print(SPLIT3.unsqueeze(-1).shape, rij_hat.shape, rij_hat.unsqueeze(1).shape)\n",
    "# #################First term\n",
    "# phiWvv = vj * SPLIT1.unsqueeze(-1).repeat(1, 1, 3)\n",
    "\n",
    "# #d_vim = scatter_sum((phiWvv + phiWvs), eij[0], dim=0)\n",
    "# d_vim = scatter_sum((phiWvv + phiWvs), eij[1], dim=0)\n",
    "# print(f\"delta v shape{d_vim.shape}\")\n",
    "\n",
    "# #d_vim = scatter_sum((phiWvv + phiWvs), eij[0], dim=0)\n",
    "# d_sim = scatter_sum(SPLIT2, eij[1], dim=0)\n",
    "# print(f\"delta s shape{d_sim.shape}\")\n",
    "\n",
    "# #vi += d_vim\n",
    "# si += d_sim\n",
    "# print(f\"sim {si.shape}\")\n",
    "# vi += d_vim\n",
    "\n",
    "# #print(f\"vi before update {vi.shape}\")\n",
    "# #print(f\"si {si.shape} si[eij[0]] {si[eij[0]].shape} si[eij[1]] {si[eij[1]].shape }\")\n",
    "\n",
    "# print(f\"vi {vi.shape}\")\n",
    "\n",
    "# Lu = nn.Sequential( nn.Linear(3, 3, bias=False) )\n",
    "# Lv = nn.Sequential( nn.Linear(3, 3, bias=False) )\n",
    "\n",
    "# Ws = Lu(vi)  \n",
    "# Wv = Lv(vi) \n",
    "# print(f\"Ws {Ws.shape}, Wv {Wv.shape}\")\n",
    "\n",
    "# V_norm = torch.norm(Ws,dim=-1)\n",
    "# print(f\"V_norm.shape {V_norm.shape}\")\n",
    "\n",
    "# STACK = torch.hstack([V_norm, si])\n",
    "# print(f\"STACK.shape {STACK.shape}\")\n",
    "\n",
    "# Llambda = nn.Sequential(\n",
    "#     Linear(in_features=256, out_features=128),\n",
    "#     SiLU(),\n",
    "#     Linear(in_features=128, out_features=256),\n",
    "# )\n",
    "# SPLITu = Llambda(STACK)\n",
    "# SPLITu1 = SPLITu[:, 0:128]\n",
    "# SPLITu2 = SPLITu[:, 128:]\n",
    "\n",
    "# d_viu = Wv * SPLITu1.unsqueeze(-1).repeat(1, 1, 3)\n",
    "# d_siu = SPLITu2\n",
    "\n",
    "# print(f\"d_siu {d_siu.shape} d_viu {d_siu.shape}\")\n",
    "# vi += d_viu\n",
    "# si += d_siu\n",
    "\n",
    "# Lr = nn.Sequential(\n",
    "#     Linear(in_features=128, out_features=64, bias=False),\n",
    "#     SiLU(),\n",
    "#     Linear(in_features=64, out_features=1, bias=False),\n",
    "# )\n",
    "# readout = Lr(si)\n",
    "# print(readout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.nn import radius_graph\n",
    "# import torch.nn as nn\n",
    "# from torch.nn import Linear, SiLU\n",
    "# from torch_scatter import scatter_sum\n",
    "# data_module = QM9DataModule(target=7)\n",
    "# data_module.prepare_data()\n",
    "# data_module.setup()\n",
    "\n",
    "# train_loader = data_module.train_dataloader()\n",
    "# for batch in train_loader:\n",
    "#     print(batch)\n",
    "#     break\n",
    "\n",
    "# ############################## compute neighbour\n",
    "# eij = radius_graph(batch.pos, r=5.0, batch=batch.batch,flow=\"source_to_target\")\n",
    "# #eij = radius_graph(batch.pos, r=5.0, batch=batch.batch,flow=\"target_to_source\")\n",
    "# #print(f\"neighbour {eij.shape}\")\n",
    "# #print(f\"neighbour {eij[0][0]}\")\n",
    "# #print(f\"neighbour {eij[0].shape}\")\n",
    "# #print(f\"neighbour {eij[1].shape}\")\n",
    "\n",
    "# ### vector distance \n",
    "# rij_vec = batch.pos[eij[0]] - batch.pos[eij[1]]\n",
    "\n",
    "# ### Norm\n",
    "# #rij_norm = torch.norm(batch.pos[eij[0]] - batch.pos[eij[1]], dim=-1, keepdim=True)\n",
    "# rij_norm = torch.norm(rij_vec, dim=-1)\n",
    "\n",
    "# ### normalization\n",
    "# rij_hat =  rij_vec / (rij_norm.unsqueeze(-1) + 1e-8)\n",
    "# def fCut(rij_norm, r_cut):\n",
    "#     f_cut = 0.5 * (torch.cos(torch.pi * rij_norm / r_cut) + 1)\n",
    "#     #print(f_cut)\n",
    "#     f_cut[rij_norm > r_cut] = 0  # Set values beyond cutoff to zero\n",
    "#     return f_cut\n",
    "\n",
    "# ### rbf \n",
    "# def fRBF(rij_norm, r_cut, n_rbf=20):\n",
    "#     t_rbf = torch.arange(1, n_rbf + 1, device=rij_norm.device).float()\n",
    "#     # Calculate RBF values\n",
    "#     rij_norm = rij_norm.unsqueeze(-1)  # Shape: [N, 1]\n",
    "    \n",
    "#     RBF = torch.sin(t_rbf * torch.pi * rij_norm / r_cut) / (rij_norm + 1e-8)\n",
    "#     # Mask for values beyond the cutoff\n",
    "#     # mask = (rij_norm <= r_cut).unsqueeze(-1)  # Shape: [N, 1]\n",
    "#     # Ws = Ws * mask.float()\n",
    "#     # Ws = Ws * fCut(rij_norm, r_cut)\n",
    "#     return RBF\n",
    "\n",
    "# RBF = fRBF(rij_norm, 5.0, 20)\n",
    "# print(f\"RBF: {RBF.shape}\")\n",
    "# ### Linear layer\n",
    "# RBF_Linear = Linear(20,384)\n",
    "# T_RBF = RBF_Linear(RBF)\n",
    "\n",
    "# Ws = T_RBF * fCut(rij_norm,5.0).unsqueeze(-1) \n",
    "\n",
    "# # print(f\"Ws.shape: {Ws.shape}\")\n",
    "\n",
    "\n",
    "# ### embeddings \n",
    "# S_embeddings = nn.Embedding(100, 128)\n",
    "# si= S_embeddings(batch['z'])\n",
    "# print(si.shape, si[eij[1]].shape,si[eij[0]].shape)\n",
    "# vi = torch.zeros_like(si).unsqueeze(-1).repeat(1, 1, 3)\n",
    "# vj = torch.zeros_like(si[eij[0]])\n",
    "# #vj = vi[eij[0]]\n",
    "# print(f\"vi {vi.shape}\")\n",
    "\n",
    "# ##################################Message block\n",
    "# ### linear layers\n",
    "# S_Linear = nn.Sequential(\n",
    "#     Linear(in_features=128,\n",
    "#         out_features=128,\n",
    "#     ),\n",
    "#     SiLU(),\n",
    "#     Linear(in_features=128,\n",
    "#         out_features=384,\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# sj = si[eij[0]]\n",
    "# print(f\"sj {sj.shape}\")\n",
    "# phi = S_Linear(sj)\n",
    "# # print(f\"phi linear shape {phi.shape}\")\n",
    "# # print(f\"phi linear {phi}\")\n",
    "\n",
    "# ### hadarmad product\n",
    "# phiW = phi * Ws\n",
    "# print(f\"vi {vi.shape} vj {vj.shape}\")\n",
    "# ### split\n",
    "# SPLIT1 = phiW[:,0:128]\n",
    "# SPLIT2 = phiW[:,128:256]\n",
    "# SPLIT3 = phiW[:,256:]\n",
    "# # print(f\"split1 {SPLIT1.shape}\")\n",
    "# # print(f\"split2 {SPLIT2.shape}\")\n",
    "# # print(f\"split3 {SPLIT3.shape}\")\n",
    "\n",
    "# ###########Second term\n",
    "# phiWvs = SPLIT3.unsqueeze(-1) * rij_hat.unsqueeze(1)\n",
    "# #print(f\"SPLIT3 {SPLIT3.shape} rij_hat {rij_hat.shape}\")\n",
    "# # print(SPLIT3.unsqueeze(-1).shape,  rij_hat.unsqueeze(1).shape)\n",
    "# #################First term\n",
    "# #phiWvv = vj * SPLIT1.unsqueeze(-1).repeat(1, 1, 3)\n",
    "# phiWvv = vj* SPLIT1\n",
    "# #print(f\"vj {vj.shape} SPLIT1 {SPLIT1.shape}\")\n",
    "\n",
    "\n",
    "# #d_vim = scatter_sum((phiWvv + phiWvs), eij[0], dim=0)\n",
    "# d_vim = scatter_sum((phiWvv.unsqueeze(-1) + phiWvs), eij[1], dim=0)\n",
    "# print(f\"delta v shape{d_vim.shape}\")\n",
    "\n",
    "# d_sim = scatter_sum(SPLIT2, eij[1], dim=0)\n",
    "# print(f\"delta s shape{d_sim.shape}\")\n",
    "\n",
    "# #vi += d_vim\n",
    "# si += d_sim\n",
    "# print(f\"sim {si.shape}\")\n",
    "# vi += d_vim\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"vi before update {vi.shape}\")\n",
    "# # print(f\"si {si.shape} si[eij[0]] {si[eij[0]].shape} si[eij[1]] {si[eij[1]].shape }\")\n",
    "\n",
    "# # print(f\"vi {vi.shape}\")\n",
    "\n",
    "# Lu = nn.Sequential( nn.Linear(384, 128, bias=False) )\n",
    "# Lv = nn.Sequential( nn.Linear(3, 3, bias=False) )\n",
    "\n",
    "# Uv = Lu(vi.view(vi.size(0), -1))  \n",
    "# Vv = Lv(vi) \n",
    "# print(f\"Uv {Uv.shape}, Vv {Vv.shape}\")\n",
    "# V_norm = torch.norm(Vv,dim=-1)\n",
    "# print(f\"V_norm.shape {V_norm.shape}\")\n",
    "\n",
    "# STACK = torch.hstack([V_norm, si])\n",
    "# print(f\"STACK.shape {STACK.shape}\")\n",
    "\n",
    "# SP = torch.sum(Uv.unsqueeze(-1) * Vv, dim=-1) \n",
    "# print(f\"SP {SP.shape}\")\n",
    "\n",
    "# Lus = nn.Sequential(\n",
    "#     Linear(in_features=256, out_features=128, bias=False),\n",
    "#     SiLU(),\n",
    "#     Linear(in_features=128, out_features=384, bias=False),\n",
    "# )\n",
    "\n",
    "# SPLITu = Lus(STACK)\n",
    "# print(f\"SPLITu {SPLITu.shape}\")\n",
    "# SPLITu1 = SPLITu[:, 0:128]\n",
    "# SPLITu2 = SPLITu[:, 128:256]\n",
    "# SPLITu3 = SPLITu[:, 256:]\n",
    "\n",
    "\n",
    "# #d_viu = Uv * SPLITu1.unsqueeze(-1).repeat(1, 1, 3)\n",
    "# d_viu = Uv * SPLITu1\n",
    "# #print(f\"SPLIT[eij[0]].unsqueeze(-1).repeat(1, 1, 3) {SPLIT[eij[0]].unsqueeze(-1).repeat(1, 1, 3).shape}\")\n",
    "# #d_siu = scatter_sum(( SP * SPLIT[eij[0]] + SPLIT[eij[0]]), eij[0], dim=0)\n",
    "# d_siu = SP * SPLITu2 + SPLITu3\n",
    "\n",
    "# #print(f\"d_siu {d_siu.shape}\")\n",
    "# vi += d_viu.unsqueeze(-1).repeat(1, 1, 3)\n",
    "# si += d_siu\n",
    "\n",
    "# Lr = nn.Sequential(\n",
    "#     Linear(in_features=128, out_features=64, bias=False),\n",
    "#     SiLU(),\n",
    "#     Linear(in_features=64, out_features=1, bias=False),\n",
    "# )\n",
    "# readout = Lr(si)\n",
    "# print(readout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import Linear, SiLU, BatchNorm1d, Dropout\n",
    "from torch_scatter import scatter_sum\n",
    "\n",
    "class Message(nn.Module):\n",
    "    def __init__(self, Ls=None, Lrbf=None, nRbf=20, nF=128):\n",
    "        super(Message, self).__init__()\n",
    "        self.Ls = Ls if Ls is not None else nn.Sequential(\n",
    "            Linear(nF, nF),\n",
    "            SiLU(),\n",
    "            Linear(nF, 3*nF),\n",
    "        )\n",
    "        self.Lrbf = Lrbf if Lrbf is not None else Linear(nRbf, 3*nF)\n",
    "\n",
    "    def fCut(self, rij_norm, rCut):\n",
    "        f_cut = 0.5 * (torch.cos(torch.pi * rij_norm / rCut) + 1)\n",
    "        f_cut[rij_norm > rCut] = 0 \n",
    "        return f_cut\n",
    "\n",
    "    def fRBF(self, rij_norm, rCut, nRbf=20):\n",
    "        Trbf = torch.arange(1, nRbf + 1, device=rij_norm.device).float()\n",
    "        rij_norm = rij_norm.unsqueeze(-1)  \n",
    "        RBF = torch.sin(Trbf * torch.pi * rij_norm / rCut) / (rij_norm + 1e-8)\n",
    "        return RBF\n",
    "\n",
    "    def forward(self, vj, sj, rij_vec, eij, rCut=5.0, nRbf=20):\n",
    "        rij_norm = torch.norm(rij_vec, dim=-1)\n",
    "        rij_hat =  rij_vec / (rij_norm.unsqueeze(-1) + 1e-8)\n",
    "\n",
    "        RBF = self.fRBF(rij_norm, rCut, nRbf)\n",
    "        T_RBF = self.Lrbf(RBF)\n",
    "        Ws = T_RBF * self.fCut(rij_norm,5.0).unsqueeze(-1) \n",
    "\n",
    "        phi = self.Ls(sj)\n",
    "        phiW = phi * Ws\n",
    "\n",
    "        SPLIT1 = phiW[:,0:128]\n",
    "        SPLIT2 = phiW[:,128:256]\n",
    "        SPLIT3 = phiW[:,256:]\n",
    "\n",
    "        phiWvv = vj * SPLIT1.unsqueeze(-1).repeat(1, 1, 3)\n",
    "        phiWvs = SPLIT3.unsqueeze(-1) * rij_hat.unsqueeze(1)\n",
    "        \n",
    "        d_vim = scatter_sum((phiWvv + phiWvs), eij[1], dim=0)\n",
    "        d_sim = scatter_sum(SPLIT2, eij[1], dim=0)\n",
    "        return d_vim, d_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# from torch.nn import Linear, SiLU\n",
    "# from torch_scatter import scatter_sum\n",
    "\n",
    "# class Message(nn.Module):\n",
    "#     def __init__(self, Ls=None, Lrbf=None, nRbf=20, nF=128):\n",
    "#         super(Message, self).__init__()\n",
    "#         self.Ls = Ls if Ls is not None else nn.Sequential(\n",
    "#             Linear(nF, nF),\n",
    "#             SiLU(),\n",
    "#             Linear(nF, 3*nF),\n",
    "#         )\n",
    "#         self.Lrbf = Lrbf if Lrbf is not None else Linear(nRbf, 3*nF)\n",
    "\n",
    "#     def fCut(self, rij_norm, rCut):\n",
    "#         f_cut = 0.5 * (torch.cos(torch.pi * rij_norm / rCut) + 1)\n",
    "#         f_cut[rij_norm > rCut] = 0 \n",
    "#         return f_cut\n",
    "\n",
    "#     def fRBF(self, rij_norm, rCut, nRbf=20):\n",
    "#         Trbf = torch.arange(1, nRbf + 1, device=rij_norm.device).float()\n",
    "#         rij_norm = rij_norm.unsqueeze(-1)  \n",
    "#         RBF = torch.sin(Trbf * torch.pi * rij_norm / rCut) / (rij_norm + 1e-8)\n",
    "#         return RBF\n",
    "\n",
    "#     def forward(self, vj, sj, rij_vec, eij, rCut=5.0, nRbf=20):\n",
    "#         rij_norm = torch.norm(rij_vec, dim=-1)\n",
    "#         rij_hat =  rij_vec / (rij_norm.unsqueeze(-1) + 1e-8)\n",
    "\n",
    "#         RBF = self.fRBF(rij_norm, rCut, nRbf)\n",
    "#         T_RBF = self.Lrbf(RBF)\n",
    "#         Ws = T_RBF * self.fCut(rij_norm,5.0).unsqueeze(-1) \n",
    "\n",
    "#         phi = self.Ls(sj)\n",
    "#         phiW = phi * Ws\n",
    "\n",
    "#         SPLIT1 = phiW[:,0:128]\n",
    "#         SPLIT2 = phiW[:,128:256]\n",
    "#         SPLIT3 = phiW[:,256:]\n",
    "\n",
    "#         phiWvv = vj * SPLIT1\n",
    "#         phiWvs = SPLIT3.unsqueeze(-1) * rij_hat.unsqueeze(1)\n",
    "        \n",
    "#         d_vim = scatter_sum((phiWvv.unsqueeze(-1) + phiWvs), eij[1], dim=0)\n",
    "#         d_sim = scatter_sum(SPLIT2, eij[1], dim=0)\n",
    "#         return d_vim, d_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Update(nn.Module):\n",
    "    def __init__(self, Luu=None, Luv=None, Ls=None, nF=128):\n",
    "        super(Update, self).__init__()\n",
    "        self.Luu = Luu if Luu is not None else Linear(3, 3, False)\n",
    "        self.Luv = Luv if Luv is not None else Linear(3, 3, False)\n",
    "        \n",
    "        self.Ls = Ls if Ls is not None else nn.Sequential(\n",
    "            Linear(in_features=2*nF, out_features=nF),\n",
    "            SiLU(),\n",
    "            Linear(in_features=nF, out_features=3*nF),\n",
    "        )\n",
    "\n",
    "    def forward(self, vi, si):\n",
    "        Uvi = self.Luu(vi) \n",
    "        Vvi = self.Luv(vi)\n",
    "\n",
    "        V_norm = torch.norm(Vvi,dim=-1)\n",
    "        STACK = torch.hstack([V_norm, si])\n",
    "\n",
    "        SP = torch.sum(Uvi * Vvi, dim=-1) \n",
    "\n",
    "        SPLIT = self.Ls(STACK)\n",
    "        SPLIT1 = SPLIT[:, 0:128]\n",
    "        SPLIT2 = SPLIT[:, 128:256]\n",
    "        SPLIT3 = SPLIT[:, 256:]\n",
    "\n",
    "        d_viu = Uvi * SPLIT1.unsqueeze(-1).repeat(1, 1, 3)\n",
    "        d_siu = SP * SPLIT2 + SPLIT3\n",
    "\n",
    "        return d_viu, d_siu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Update(nn.Module):\n",
    "#     def __init__(self, Luu=None, Luv=None, Ls=None):\n",
    "#         super(Update, self).__init__()\n",
    "#         self.Luu = Luu if Luu is not None else Linear(3, 3, False)\n",
    "#         self.Luv = Luv if Luv is not None else Linear(3, 3, False)\n",
    "        \n",
    "#         self.Ls = Ls if Ls is not None else nn.Sequential(\n",
    "#             Linear(in_features=256, out_features=128),\n",
    "#             SiLU(),\n",
    "#             Linear(in_features=128, out_features=256),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, vi, si):\n",
    "#         Uvi = self.Luu(vi) \n",
    "#         Vvi = self.Luv(vi)\n",
    "\n",
    "#         V_norm = torch.norm(Vvi,dim=-1)\n",
    "#         STACK = torch.hstack([V_norm, si])\n",
    "\n",
    "#         SPLIT = self.Ls(STACK)\n",
    "#         SPLIT1 = SPLIT[:, 0:128]\n",
    "#         SPLIT2 = SPLIT[:, 128:]\n",
    "\n",
    "#         d_viu = Uvi * SPLIT1.unsqueeze(-1).repeat(1, 1, 3)\n",
    "#         d_siu = SPLIT2\n",
    "\n",
    "#         return d_viu, d_siu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Update(nn.Module):\n",
    "#     def __init__(self, Luu=None, Luv=None, Ls=None, nF=128):\n",
    "#         super(Update, self).__init__()\n",
    "#         self.Luu = Luu if Luu is not None else Linear(384, 128, False)\n",
    "#         self.Luv = Luv if Luv is not None else Linear(3, 3, False)\n",
    "        \n",
    "#         self.Ls = Ls if Ls is not None else nn.Sequential(\n",
    "#             Linear(in_features=2*nF, out_features=nF),\n",
    "#             SiLU(),\n",
    "#             Linear(in_features=nF, out_features=3*nF),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, vi, si):\n",
    "#         Uvi = self.Luu(vi.view(vi.size(0), -1)) \n",
    "#         Vvi = self.Luv(vi)\n",
    "\n",
    "#         V_norm = torch.norm(Vvi,dim=-1)\n",
    "#         STACK = torch.hstack([V_norm, si])\n",
    "\n",
    "#         SP = torch.sum(Uvi.unsqueeze(-1) * Vvi, dim=-1) \n",
    "\n",
    "#         SPLIT = self.Ls(STACK)\n",
    "#         SPLIT1 = SPLIT[:, 0:128]\n",
    "#         SPLIT2 = SPLIT[:, 128:256]\n",
    "#         SPLIT3 = SPLIT[:, 256:]\n",
    "\n",
    "#         d_viu = Uvi * SPLIT1\n",
    "#         d_siu = SP * SPLIT2 + SPLIT3\n",
    "\n",
    "#         return d_viu.unsqueeze(-1).repeat(1, 1, 3), d_siu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import radius_graph\n",
    "\n",
    "class PaiNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Polarizable Atom Interaction Neural Network with PyTorch.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, Lm, Lu,\n",
    "        num_message_passing_layers: int = 3,\n",
    "        num_features: int = 128,\n",
    "        num_outputs: int = 1,\n",
    "        num_rbf_features: int = 20,\n",
    "        num_unique_atoms: int = 100,\n",
    "        cutoff_dist: float = 5.0,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_message_passing_layers: Number of message passing layers in\n",
    "                the PaiNN model.\n",
    "            num_features: Size of the node embeddings (scalar features) and\n",
    "                vector features.\n",
    "            num_outputs: Number of model outputs. In most cases 1.\n",
    "            num_rbf_features: Number of radial basis functions to represent\n",
    "                distances.\n",
    "            num_unique_atoms: Number of unique atoms in the data that we want\n",
    "                to learn embeddings for.\n",
    "            cutoff_dist: Euclidean distance threshold for determining whether \n",
    "                two nodes (atoms) are neighbours.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        #raise NotImplementedError\n",
    "        self.num_message_passing_layers = num_message_passing_layers\n",
    "        self.num_features = num_features\n",
    "        self.num_outputs = num_outputs\n",
    "        self.num_rbf_features = num_rbf_features\n",
    "        self.num_unique_atoms = num_unique_atoms\n",
    "        self.cutoff_dist = cutoff_dist\n",
    "\n",
    "        self.zi = nn.Embedding(num_unique_atoms, num_features)\n",
    "\n",
    "        self.Lm = Lm\n",
    "        self.Lu = Lu\n",
    "\n",
    "        self.Lr = nn.Sequential(\n",
    "            Linear(in_features=128, out_features=32),\n",
    "            SiLU(),\n",
    "            Linear(in_features=32, out_features=1),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        atoms: torch.LongTensor,\n",
    "        atom_positions: torch.FloatTensor,\n",
    "        graph_indexes: torch.LongTensor,\n",
    "    ) -> torch.FloatTensor:\n",
    "        si = self.zi(atoms)\n",
    "        eij = radius_graph(atom_positions, r=self.cutoff_dist, batch=graph_indexes)\n",
    "        sj = si[eij[0]]\n",
    "        vi = torch.zeros_like(si).unsqueeze(-1).repeat(1, 1, 3)\n",
    "        vj = vi[eij[0]]\n",
    "        #vj = torch.zeros_like(si[eij[0]])\n",
    "        rij_vec = atom_positions[eij[0]] - atom_positions[eij[1]]\n",
    "        for _ in range(self.num_message_passing_layers):\n",
    "            d_vim, d_sim = self.Lm(vj, sj, rij_vec, eij)\n",
    "            vi = vi + d_vim\n",
    "            si = si + d_sim\n",
    "\n",
    "            d_viu, d_siu = self.Lu(vi, si)\n",
    "\n",
    "            vi = vi + d_viu\n",
    "            si = si + d_siu\n",
    "        \n",
    "        Sigma = self.Lr(si)\n",
    "\n",
    "        return Sigma\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cli(args: list = []):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--seed', default=0)\n",
    "\n",
    "    # Data\n",
    "    parser.add_argument('--target', default=7, type=int) # 7 => Internal energy at 0K\n",
    "    parser.add_argument('--data_dir', default='data/', type=str)\n",
    "    parser.add_argument('--batch_size_train', default=100, type=int)\n",
    "    parser.add_argument('--batch_size_inference', default=1000, type=int)\n",
    "    parser.add_argument('--num_workers', default=0, type=int)\n",
    "    parser.add_argument('--splits', nargs=3, default=[110000, 10000, 10831], type=int) # [num_train, num_val, num_test]\n",
    "    parser.add_argument('--subset_size', default=None, type=int)\n",
    "\n",
    "    # Model\n",
    "    parser.add_argument('--num_message_passing_layers', default=3, type=int)\n",
    "    parser.add_argument('--num_features', default=128, type=int)\n",
    "    parser.add_argument('--num_outputs', default=1, type=int)\n",
    "    parser.add_argument('--num_rbf_features', default=20, type=int)\n",
    "    parser.add_argument('--num_unique_atoms', default=100, type=int)\n",
    "    parser.add_argument('--cutoff_dist', default=5.0, type=float)\n",
    "\n",
    "    # Training\n",
    "    parser.add_argument('--lr', default=5e-4, type=float)\n",
    "    #parser.add_argument('--weight_decay', default=0.01, type=float)\n",
    "    #parser.add_argument('--weight_decay', default=1e-8, type=float)\n",
    "    #parser.add_argument('--weight_decay', default=1e-3, type=float)\n",
    "    parser.add_argument('--weight_decay', default=1e-2, type=float)\n",
    "    parser.add_argument('--num_epochs', default=1000, type=int)\n",
    "\n",
    "    args = parser.parse_args(args=args)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [] # Specify non-default arguments in this list\n",
    "args = cli(args)\n",
    "seed_everything(args.seed)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "device_name = torch.cuda.get_device_name(torch.cuda.current_device())\n",
    "print(device_name)\n",
    "dm = QM9DataModule(\n",
    "    target=args.target,\n",
    "    data_dir=args.data_dir,\n",
    "    batch_size_train=args.batch_size_train,\n",
    "    batch_size_inference=args.batch_size_inference,\n",
    "    num_workers=args.num_workers,\n",
    "    splits=args.splits,\n",
    "    seed=args.seed,\n",
    "    subset_size=args.subset_size,\n",
    ")\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "y_mean, y_std, atom_refs = dm.get_target_stats(\n",
    "    remove_atom_refs=True, divide_by_atoms=True\n",
    ")\n",
    "\n",
    "painn = PaiNN(\n",
    "    Lm=Message(),\n",
    "    Lu=Update(),\n",
    "    num_message_passing_layers=args.num_message_passing_layers,\n",
    "    num_features=args.num_features,\n",
    "    num_outputs=args.num_outputs, \n",
    "    num_rbf_features=args.num_rbf_features,\n",
    "    num_unique_atoms=args.num_unique_atoms,\n",
    "    cutoff_dist=args.cutoff_dist,\n",
    ")\n",
    "post_processing = AtomwisePostProcessing(\n",
    "    args.num_outputs, y_mean, y_std, atom_refs\n",
    ")\n",
    "\n",
    "painn.to(device)\n",
    "post_processing.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    painn.parameters(),\n",
    "    lr=args.lr,\n",
    "    weight_decay=args.weight_decay,\n",
    ")\n",
    "\n",
    "\n",
    "train_losses, val_losses, val_maes = [], [], []\n",
    "best_val_loss = float('inf')\n",
    "patience = 30  # Number of epochs to wait before stopping\n",
    "\n",
    "smoothed_val_loss = None\n",
    "smoothing_factor = 0.9\n",
    "wait = 0\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=5, threshold=1e-4\n",
    ")\n",
    "\n",
    "painn.train()\n",
    "pbar = trange(args.num_epochs)\n",
    "for epoch in pbar:\n",
    "#for epoch in range(args.num_epochs):\n",
    "\n",
    "    loss_epoch = 0.\n",
    "    for batch in dm.train_dataloader():\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        atomic_contributions = painn(\n",
    "            atoms=batch.z,\n",
    "            atom_positions=batch.pos,\n",
    "            graph_indexes=batch.batch\n",
    "        )\n",
    "        preds = post_processing(\n",
    "            atoms=batch.z,\n",
    "            graph_indexes=batch.batch,\n",
    "            atomic_contributions=atomic_contributions,\n",
    "        )\n",
    "        loss_step = F.mse_loss(preds, batch.y, reduction='sum')\n",
    "        loss = loss_step / len(batch.y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_epoch += loss_step.detach().item()\n",
    "\n",
    "    loss_epoch /= len(dm.data_train)\n",
    "    train_losses.append(loss_epoch)\n",
    "\n",
    "    # Validation Loop\n",
    "    painn.eval()\n",
    "    val_loss_epoch = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in dm.val_dataloader():\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            atomic_contributions = painn(\n",
    "                atoms=batch.z,\n",
    "                atom_positions=batch.pos,\n",
    "                graph_indexes=batch.batch,\n",
    "            )\n",
    "            preds = post_processing(\n",
    "                atoms=batch.z,\n",
    "                graph_indexes=batch.batch,\n",
    "                atomic_contributions=atomic_contributions,\n",
    "            )\n",
    "            val_loss_step = F.mse_loss(preds, batch.y, reduction='sum')\n",
    "            val_loss_epoch += val_loss_step.item()\n",
    "\n",
    "    val_loss_epoch /= len(dm.data_val)\n",
    "    val_losses.append(val_loss_epoch)\n",
    "\n",
    "    if smoothed_val_loss is None:\n",
    "        smoothed_val_loss = val_loss_epoch\n",
    "    else:\n",
    "        smoothed_val_loss = smoothing_factor * val_loss_epoch + (1 - smoothing_factor) * smoothed_val_loss\n",
    "\n",
    "    # Early Stopping\n",
    "    if smoothed_val_loss < best_val_loss:\n",
    "        best_val_loss = smoothed_val_loss\n",
    "        wait = 0  # Reset the patience counter\n",
    "        torch.save(painn.state_dict(), \"better_painn.pth\")  # Save the best model\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "            break\n",
    "\n",
    "    current_lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "    pbar.set_postfix_str(f\"Epoch: {epoch + 1}\\tTL: {loss_epoch:.3e}\\tVL: {val_loss_epoch:.3e}\\tLR:{current_lr}\")\n",
    "    scheduler.step(smoothed_val_loss)\n",
    "    #print(f\"Epoch: {epoch + 1}\\tTL: {loss_epoch:.3e}\\tVL: {val_loss_epoch:.3e}\\tLR:{current_lr}\")\n",
    "\n",
    "\n",
    "painn.load_state_dict(torch.load(\"better_painn.pth\", weights_only=True))\n",
    "mae = 0\n",
    "painn.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in dm.test_dataloader():\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        atomic_contributions = painn(\n",
    "            atoms=batch.z,\n",
    "            atom_positions=batch.pos,\n",
    "            graph_indexes=batch.batch,\n",
    "        )\n",
    "        preds = post_processing(\n",
    "            atoms=batch.z,\n",
    "            graph_indexes=batch.batch,\n",
    "            atomic_contributions=atomic_contributions,\n",
    "        )\n",
    "        mae += F.l1_loss(preds, batch.y, reduction='sum')\n",
    "\n",
    "mae /= len(dm.data_test)\n",
    "unit_conversion = dm.unit_conversion[args.target]\n",
    "print(f'Test MAE: {unit_conversion(mae):.3f}')\n",
    "\n",
    "# Plot Training and Validation Metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Metrics\")\n",
    "#plt.show()\n",
    "plt.savefig(\"train_val_loss.png\")\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "painn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
