{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02456 Molecular Property Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic example of how to train the PaiNN model to predict the QM9 property\n",
    "\"internal energy at 0K\". This property (and the majority of the other QM9\n",
    "properties) is computed as a sum of atomic contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "from tqdm import trange\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import seed_everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QM9 Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.loader import DataLoader\n",
    "from typing import Optional, List, Union, Tuple\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "\n",
    "\n",
    "class GetTarget(BaseTransform):\n",
    "    def __init__(self, target: Optional[int] = None) -> None:\n",
    "        self.target = [target]\n",
    "\n",
    "\n",
    "    def forward(self, data: Data) -> Data:\n",
    "        if self.target is not None:\n",
    "            data.y = data.y[:, self.target]\n",
    "        return data\n",
    "\n",
    "\n",
    "class QM9DataModule(pl.LightningDataModule):\n",
    "\n",
    "    target_types = ['atomwise' for _ in range(19)]\n",
    "    target_types[0] = 'dipole_moment'\n",
    "    target_types[5] = 'electronic_spatial_extent'\n",
    "\n",
    "    # Specify unit conversions (eV to meV).\n",
    "    unit_conversion = {\n",
    "        i: (lambda t: 1000*t) if i not in [0, 1, 5, 11, 16, 17, 18]\n",
    "        else (lambda t: t)\n",
    "        for i in range(19)\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target: int = 7,\n",
    "        data_dir: str = 'data/',\n",
    "        batch_size_train: int = 100,\n",
    "        batch_size_inference: int = 1000,\n",
    "        num_workers: int = 0,\n",
    "        splits: Union[List[int], List[float]] = [110000, 10000, 10831],\n",
    "        seed: int = 0,\n",
    "        subset_size: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.target = target\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size_train = batch_size_train\n",
    "        self.batch_size_inference = batch_size_inference\n",
    "        self.num_workers = num_workers\n",
    "        self.splits = splits\n",
    "        self.seed = seed\n",
    "        self.subset_size = subset_size\n",
    "\n",
    "        self.data_train = None\n",
    "        self.data_val = None\n",
    "        self.data_test = None\n",
    "\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # Download data\n",
    "        QM9(root=self.data_dir)\n",
    "\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        dataset = QM9(root=self.data_dir, transform=GetTarget(self.target))\n",
    "\n",
    "        # Shuffle dataset\n",
    "        rng = np.random.default_rng(seed=self.seed)\n",
    "        dataset = dataset[rng.permutation(len(dataset))]\n",
    "\n",
    "        # Subset dataset\n",
    "        if self.subset_size is not None:\n",
    "            dataset = dataset[:self.subset_size]\n",
    "        \n",
    "        # Split dataset\n",
    "        if all([type(split) == int for split in self.splits]):\n",
    "            split_sizes = self.splits\n",
    "        elif all([type(split) == float for split in self.splits]):\n",
    "            split_sizes = [int(len(dataset) * prop) for prop in self.splits]\n",
    "\n",
    "        split_idx = np.cumsum(split_sizes)\n",
    "        self.data_train = dataset[:split_idx[0]]\n",
    "        self.data_val = dataset[split_idx[0]:split_idx[1]]\n",
    "        self.data_test = dataset[split_idx[1]:]\n",
    "\n",
    "\n",
    "    def get_target_stats(\n",
    "        self,\n",
    "        remove_atom_refs: bool = True,\n",
    "        divide_by_atoms: bool = True\n",
    "    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n",
    "        atom_refs = self.data_train.atomref(self.target)\n",
    "\n",
    "        ys = list()\n",
    "        for batch in self.train_dataloader(shuffle=False):\n",
    "            y = batch.y.clone()\n",
    "            if remove_atom_refs and atom_refs is not None:\n",
    "                y.index_add_(\n",
    "                    dim=0, index=batch.batch, source=-atom_refs[batch.z]\n",
    "                )\n",
    "            if divide_by_atoms:\n",
    "                _, num_atoms  = torch.unique(batch.batch, return_counts=True)\n",
    "                y = y / num_atoms.unsqueeze(-1)\n",
    "            ys.append(y)\n",
    "\n",
    "        y = torch.cat(ys, dim=0)\n",
    "        return y.mean(), y.std(), atom_refs\n",
    "\n",
    "\n",
    "    def train_dataloader(self, shuffle: bool = True) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.data_train,\n",
    "            batch_size=self.batch_size_train,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=shuffle,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.data_val,\n",
    "            batch_size=self.batch_size_inference,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.data_test,\n",
    "            batch_size=self.batch_size_inference,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class AtomwisePostProcessing(nn.Module):\n",
    "    \"\"\"\n",
    "    Post-processing for (QM9) properties that are predicted as sums of atomic\n",
    "    contributions.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_outputs: int,\n",
    "        mean: torch.FloatTensor,\n",
    "        std: torch.FloatTensor,\n",
    "        atom_refs: torch.FloatTensor,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_outputs: Integer with the number of model outputs. In most\n",
    "                cases 1.\n",
    "            mean: torch.FloatTensor with mean value to shift atomwise\n",
    "                contributions by.\n",
    "            std: torch.FloatTensor with standard deviation to scale atomwise\n",
    "                contributions by.\n",
    "            atom_refs: torch.FloatTensor of size [num_atom_types, 1] with\n",
    "                atomic reference values.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "        self.register_buffer('scale', std)\n",
    "        self.register_buffer('shift', mean)\n",
    "        self.atom_refs = nn.Embedding.from_pretrained(atom_refs, freeze=True)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        atomic_contributions: torch.FloatTensor,\n",
    "        atoms: torch.LongTensor,\n",
    "        graph_indexes: torch.LongTensor,\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Atomwise post-processing operations and atomic sum.\n",
    "\n",
    "        Args:\n",
    "            atomic_contributions: torch.FloatTensor of size [num_nodes,\n",
    "                num_outputs] with each node's contribution to the overall graph\n",
    "                prediction, i.e., each atom's contribution to the overall\n",
    "                molecular property prediction.\n",
    "            atoms: torch.LongTensor of size [num_nodes] with atom type of each\n",
    "                node in the graph.\n",
    "            graph_indexes: torch.LongTensor of size [num_nodes] with the graph \n",
    "                index each node belongs to.\n",
    "\n",
    "        Returns:\n",
    "            A torch.FLoatTensor of size [num_graphs, num_outputs] with\n",
    "            predictions for each graph (molecule).\n",
    "        \"\"\"\n",
    "        num_graphs = torch.unique(graph_indexes).shape[0]\n",
    "\n",
    "        atomic_contributions = atomic_contributions*self.scale + self.shift\n",
    "        atomic_contributions = atomic_contributions + self.atom_refs(atoms)\n",
    "\n",
    "        # Sum contributions for each graph\n",
    "        output_per_graph = torch.zeros(\n",
    "            (num_graphs, self.num_outputs),\n",
    "            device=atomic_contributions.device,\n",
    "        )\n",
    "        output_per_graph.index_add_(\n",
    "            dim=0,\n",
    "            index=graph_indexes,\n",
    "            source=atomic_contributions,\n",
    "        )\n",
    "\n",
    "        return output_per_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PaiNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1. Compute Scala Messages\n",
    "\n",
    "\\begin{align*}\n",
    "    m_{ij} \\ \\text{or} \\ h^n  & = \\phi_m (x_i, \\ x_j, \\ || \\vec{d}_{ij} || ) = \\mathsf{MLP}( [ x_i, x_j, || \\vec{d}_{ij} || ]) \\\\\n",
    "           \\Rightarrow M_i & = \\sum_{j \\in \\mathcal{N}(i) } m_{ij} x_j \\cdot \\vec{d}_{ij} \\ \\text{ aggregation }\\\\\n",
    "           \\Rightarrow x' & =  \\phi_m (x_i, M_i ) \\ \\text{ update }  \\\\  \n",
    "           & = x_i + M_i \n",
    "\\end{align*}\n",
    "\n",
    "* e.g. \n",
    "\n",
    "\\begin{align*}\n",
    "    m_{A} & = \\phi (x_A, \\ x_B, || \\vec{d}_{AB} || ) \\\\\n",
    "           & = \\mathsf{MLP}([0.5, 1.2, 0.8, 0.9, \\sqrt{2} ]) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "##### Note. Displacement magnitude\n",
    "\n",
    "\\begin{align*}\n",
    "    || \\vec{d}_{AB} || & = \\vec{r}_{A} - \\vec{r}_{B} \\\\\n",
    "    & = \\sqrt{ (-1)^2 + (1)^2 + (0)^2 } || \\\\\n",
    "    & = \\sqrt{2}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "We'll start with the following setting for the MLP, 2 laye network, input size 5, hidden size 4 and ouput size 2.\n",
    "\n",
    "1.3 Linear layer(Linear compbination) \n",
    "\n",
    "* e.g., \n",
    "\\begin{align*}\n",
    "    h^n = w_n m_i + b_n\n",
    "\\end{align*}\n",
    "\n",
    "1.4. Initialize weights and biases, typically they are initalized radomly.\n",
    "\n",
    "e.g.,\n",
    "\n",
    "\\begin{align*}\n",
    "    h^1 = [0.996,0.802,âˆ’0.168,0.009]\n",
    "\\end{align*}\n",
    "\n",
    "1.5. Apply activation function SiLU\n",
    "\n",
    "    SiLU(h^1) = ?\n",
    "\n",
    "1.6 Apply SiLU(h^1) to next connected layer(s)\n",
    "e.g.\n",
    "\\begin{align*}\n",
    "    m_{AB} \\text{ or } (h^2) = w_2 \\text{}(h^1) + b_2\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "2. Compute Vectorial Messages\n",
    "\n",
    "Vectorial messages are just matrix version of the function above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Message Block:\n",
    "Features-wise continuous-filterlter convolutions:\n",
    "\\begin{align*}\n",
    "    \\Delta {\\mathrm{s}_i}^{m} = & ( \\phi_s ( \\mathrm{s} ) * \\mathcal{W}_s )_i \\\\\n",
    "        = & \\sum_j \\phi_s ( \\mathrm{s}_j ) \\circ \\mathcal{W}_s ( || \\vec{r}_{ij} || )\n",
    "\\end{align*}\n",
    "\n",
    "The rotationally-invariant filter $\\mathcal{W}_s$ are linear combinations of radial basis function :\n",
    "\\begin{align*}\n",
    "    \\mathcal{W}_s ( || \\vec{r}_{ij} || ) = \\text{sin} (\\frac{n\\pi}{r_{cut}} || \\vec{r}_{ij}||) /  || \\vec{r}_{ij}||\n",
    "\\end{align*}\n",
    "\n",
    "cutoff function:\n",
    "\\begin{align}\n",
    "f_{cos}( || \\vec{r}_{ij} || ) & =\n",
    "    \\begin{cases}\n",
    "      0.5 \\cdot \\left( \\text{cos} \\left( \\frac { \\pi \\vec{r}_{ij} }{ r_{ \\text{cut} } }  \\right) \\right) & \\text{if} \\ || \\vec{r}_{ij} || \\leq r_{ \\text{cut} } \\\\\n",
    "      \\\\\n",
    "      0  & \\text{otherwise}\n",
    "    \\end{cases}    \\\\\n",
    "\\end{align}\n",
    "\n",
    "Continuous-filter convolutions for the residual of the equivariant message function\n",
    "\n",
    "\\begin{align*}\n",
    "    \\Delta \\vec{ \\mathrm{v}_i }^{m} = & \\sum_j \\vec{ \\mathrm{v}_j } \\circ \\phi_{vv} ( \\text{s}_j ) \\circ \\mathcal{ W }_{vv} ( || \\vec{r}_{ij} || ) \\\\\n",
    "        + & \\sum_j \\phi_{vs} ( \\text{s}_j ) \\circ \\mathcal{W}'_{vs} ( || \\vec{r}_{ij} || ) \\frac{ \\vec{r}_{ij} }{ || \\vec{r}_{ij} || } \n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Update Block:\n",
    "Atomwise udate accross features after the features-wise message passing, the residual of the scalar update function:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\Delta \\mathrm{s}_i^{u} = & \\mathrm{a}_{ss} ( \\mathrm{s}_i, || \\mathrm{V} \\mathrm{ \\vec{v} }_i || ) \\\\\n",
    "    + & \\mathrm{a}_{sv} ( \\mathrm{s}_i, || \\mathrm{V} \\mathrm{ \\vec{v} }_i || ) \\langle \\mathrm{U} \\mathrm{ \\vec{v}_i }, \\mathrm{V} \\mathrm{ \\vec{v}_i } \\rangle\n",
    "\\end{align*}\n",
    "\n",
    "Equivariant features\n",
    "\n",
    "\\begin{align*}\n",
    "    \\Delta \\mathrm{ \\vec{v} }_i^{u} = & \\mathrm{a}_{vv} ( \\mathrm{s}_i, || \\mathrm{V} \\mathrm{ \\vec{v} }_i || ) \\mathrm{U} \\mathrm{ \\vec{v} }_i\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.nn import radius_graph\n",
    "# data_module = QM9DataModule(target=7)\n",
    "# data_module.prepare_data()\n",
    "# data_module.setup()\n",
    "\n",
    "# train_loader = data_module.train_dataloader()\n",
    "# for batch in train_loader:\n",
    "#     print(batch)\n",
    "#     break\n",
    "\n",
    "# ############################## compute neighbour\n",
    "# edgeij = radius_graph(batch.pos, r=5.0, batch=batch.batch)\n",
    "# print(f\"edgeij {edgeij[0]} edgeij {edgeij[1]}\")\n",
    "# eij = radius_graph(batch.pos, r=5.0, batch=batch.batch,flow=\"source_to_target\")\n",
    "# print(f\"eij {eij[0]} eij {eij[1]}\")\n",
    "# eji = radius_graph(batch.pos, r=5.0, batch=batch.batch,flow=\"target_to_source\")\n",
    "# print(f\"eji {eji[0]} eji {eji[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[1810, 11], edge_index=[2, 3714], edge_attr=[3714, 4], y=[100, 1], pos=[1810, 3], idx=[100], name=[100], z=[1810], batch=[1810], ptr=[101])\n",
      "RBF: torch.Size([28164, 20])\n",
      "sj torch.Size([28164, 128])\n",
      "vi torch.Size([1810, 128, 3]) vj torch.Size([28164, 128, 3])\n",
      "torch.Size([28164, 128, 1]) torch.Size([28164, 3]) torch.Size([28164, 1, 3])\n",
      "delta v shapetorch.Size([1810, 128, 3])\n",
      "delta s shapetorch.Size([1810, 128])\n",
      "sim torch.Size([1810, 128])\n",
      "vi torch.Size([1810, 128, 3])\n",
      "Uv torch.Size([1810, 128, 3]), Vv torch.Size([1810, 128, 3])\n",
      "SP torch.Size([1810, 128])\n",
      "tensor([[-0.2238],\n",
      "        [-0.2118],\n",
      "        [ 0.0289],\n",
      "        ...,\n",
      "        [ 0.1271],\n",
      "        [ 0.1240],\n",
      "        [ 0.1195]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import radius_graph\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, SiLU\n",
    "from torch_scatter import scatter_sum\n",
    "data_module = QM9DataModule(target=7)\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "\n",
    "train_loader = data_module.train_dataloader()\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break\n",
    "\n",
    "############################## compute neighbour\n",
    "eij = radius_graph(batch.pos, r=5.0, batch=batch.batch,flow=\"source_to_target\")\n",
    "#eij = radius_graph(batch.pos, r=5.0, batch=batch.batch,flow=\"target_to_source\")\n",
    "#print(f\"neighbour {eij.shape}\")\n",
    "#print(f\"neighbour {eij[0][0]}\")\n",
    "#print(f\"neighbour {eij[0].shape}\")\n",
    "#print(f\"neighbour {eij[1].shape}\")\n",
    "\n",
    "### vector distance \n",
    "rij_vec = batch.pos[eij[0]] - batch.pos[eij[1]]\n",
    "\n",
    "### Norm\n",
    "#rij_norm = torch.norm(batch.pos[eij[0]] - batch.pos[eij[1]], dim=-1, keepdim=True)\n",
    "rij_norm = torch.norm(rij_vec, dim=-1)\n",
    "\n",
    "### normalization\n",
    "rij_hat =  rij_vec / (rij_norm.unsqueeze(-1) + 1e-8)\n",
    "\n",
    "def fCut(rij_norm, r_cut):\n",
    "    f_cut = 0.5 * (torch.cos(torch.pi * rij_norm / r_cut) + 1)\n",
    "    #print(f_cut)\n",
    "    f_cut[rij_norm > r_cut] = 0  # Set values beyond cutoff to zero\n",
    "    return f_cut\n",
    "\n",
    "### rbf \n",
    "def fRBF(rij_norm, r_cut, n_rbf=20):\n",
    "    t_rbf = torch.arange(1, n_rbf + 1, device=rij_norm.device).float()\n",
    "    # Calculate RBF values\n",
    "    rij_norm = rij_norm.unsqueeze(-1)  # Shape: [N, 1]\n",
    "    \n",
    "    RBF = torch.sin(t_rbf * torch.pi * rij_norm / r_cut) / (rij_norm + 1e-8)\n",
    "    # Mask for values beyond the cutoff\n",
    "    # mask = (rij_norm <= r_cut).unsqueeze(-1)  # Shape: [N, 1]\n",
    "    # Ws = Ws * mask.float()\n",
    "    # Ws = Ws * fCut(rij_norm, r_cut)\n",
    "    return RBF\n",
    "\n",
    "RBF = fRBF(rij_norm, 5.0, 20)\n",
    "print(f\"RBF: {RBF.shape}\")\n",
    "### Linear layer\n",
    "RBF_Linear = Linear(20,384)\n",
    "T_RBF = RBF_Linear(RBF)\n",
    "\n",
    "Ws = T_RBF * fCut(rij_norm,5.0).unsqueeze(-1) \n",
    "\n",
    "# print(f\"Ws.shape: {Ws.shape}\")\n",
    "\n",
    "\n",
    "### embeddings \n",
    "S_embeddings = nn.Embedding(100, 128)\n",
    "si= S_embeddings(batch['z'])\n",
    "#print(si.shape, si[eij[1]].shape,si[eij[0]].shape)\n",
    "vi = torch.zeros_like(si).unsqueeze(-1).repeat(1, 1, 3)\n",
    "\n",
    "##################################Message block\n",
    "### linear layers\n",
    "S_Linear = nn.Sequential(\n",
    "    Linear(in_features=128,\n",
    "        out_features=128,\n",
    "    ),\n",
    "    SiLU(),\n",
    "    Linear(in_features=128,\n",
    "        out_features=384,\n",
    "    ),\n",
    ")\n",
    "\n",
    "#sj = si[eij[1]]\n",
    "sj = si[eij[0]]\n",
    "print(f\"sj {sj.shape}\")\n",
    "phi = S_Linear(sj)\n",
    "# print(f\"phi linear shape {phi.shape}\")\n",
    "# print(f\"phi linear {phi}\")\n",
    "\n",
    "#vj = torch.zeros_like(si[eij[1]]).unsqueeze(-1)\n",
    "#vj = vi[eij[1]]\n",
    "vj = vi[eij[0]]\n",
    "### hadarmad product\n",
    "phiW = phi * Ws\n",
    "print(f\"vi {vi.shape} vj {vj.shape}\")\n",
    "### split\n",
    "#Split_Linear = Linear(in_features=384, out_features=128, bias=False)\n",
    "\n",
    "#Split = Split_Linear(phiW)\n",
    "SPLIT1 = phiW[:,0:128]\n",
    "SPLIT2 = phiW[:,128:256]\n",
    "SPLIT3 = phiW[:,256:]\n",
    "# print(f\"split1 {SPLIT1.shape}\")\n",
    "# print(f\"split2 {SPLIT2.shape}\")\n",
    "# print(f\"split3 {SPLIT3.shape}\")\n",
    "\n",
    "###########Second term\n",
    "phiWvs = SPLIT3.unsqueeze(-1) * rij_hat.unsqueeze(1)\n",
    "print(SPLIT3.unsqueeze(-1).shape, rij_hat.shape, rij_hat.unsqueeze(1).shape)\n",
    "#################First term\n",
    "phiWvv = vj * SPLIT1.unsqueeze(-1).repeat(1, 1, 3)\n",
    "\n",
    "#d_vim = scatter_sum((phiWvv + phiWvs), eij[0], dim=0)\n",
    "d_vim = scatter_sum((phiWvv + phiWvs), eij[1], dim=0)\n",
    "print(f\"delta v shape{d_vim.shape}\")\n",
    "\n",
    "#d_vim = scatter_sum((phiWvv + phiWvs), eij[0], dim=0)\n",
    "d_sim = scatter_sum(SPLIT2, eij[1], dim=0)\n",
    "print(f\"delta s shape{d_sim.shape}\")\n",
    "\n",
    "#vi += d_vim\n",
    "si += d_sim\n",
    "print(f\"sim {si.shape}\")\n",
    "vi += d_vim\n",
    "\n",
    "\n",
    "#print(f\"vi before update {vi.shape}\")\n",
    "#print(f\"si {si.shape} si[eij[0]] {si[eij[0]].shape} si[eij[1]] {si[eij[1]].shape }\")\n",
    "#sj = si[eij[1]]\n",
    "#vj = vi[eij[1]]\n",
    "print(f\"vi {vi.shape}\")\n",
    "Luu = nn.Sequential( nn.Linear(3, 3, bias=False) )\n",
    "Luv = nn.Sequential( nn.Linear(3, 3, bias=False) )\n",
    "# Uvj = Luu(vj)  # Learnable weights for U\n",
    "# Vvj = Luv(vj)  # Learnable weights for V\n",
    "Uv = Luu(vi)  \n",
    "Vv = Luv(vi) \n",
    "print(f\"Uv {Uv.shape}, Vv {Vv.shape}\")\n",
    "\n",
    "#cnn\n",
    "# Cu = nn.Sequential(\n",
    "#     nn.Conv1d(in_channels=3, out_channels=1, kernel_size=1, stride=1),\n",
    "# )\n",
    "\n",
    "# # Reshape for Conv1d compatibility\n",
    "# V = Cu(vj.permute(0, 2, 1)).squeeze(1)  # Shape: [M, 1, 128] to Shape: [M, 128]\n",
    "# print(V)\n",
    "# V_norm = torch.norm(V,dim=-1)\n",
    "\n",
    "#V_norm = torch.norm(Vvj,dim=-1)\n",
    "V_norm = torch.norm(Vv,dim=-1)\n",
    "# print(f\"V_norm.shape {V_norm.shape}\")\n",
    "\n",
    "#STACK = torch.hstack([V_norm, sj])\n",
    "STACK = torch.hstack([V_norm, si])\n",
    "#print(f\"STACK.shape {STACK.shape}\")\n",
    "\n",
    "#SP = torch.sum(Uvj * Vvj, dim=-1) \n",
    "SP = torch.sum(Uv * Vv, dim=-1) \n",
    "print(f\"SP {SP.shape}\")\n",
    "\n",
    "Lus = nn.Sequential(\n",
    "    Linear(in_features=256, out_features=128, bias=False),\n",
    "    SiLU(),\n",
    "    Linear(in_features=128, out_features=384, bias=False),\n",
    "    #Linear(in_features=384, out_features=128, bias=False),\n",
    ")\n",
    "\n",
    "SPLITu = Lus(STACK)\n",
    "SPLITu1 = SPLITu[:, 0:128]\n",
    "SPLITu2 = SPLITu[:, 128:256]\n",
    "SPLITu3 = SPLITu[:, 256:]\n",
    "\n",
    "#d_viu = scatter_sum((Uvj * SPLIT.unsqueeze(-1).repeat(1, 1, 3)), eij[0], dim=0)\n",
    "d_viu = Uv * SPLITu1.unsqueeze(-1).repeat(1, 1, 3)\n",
    "#print(f\"SPLIT[eij[0]].unsqueeze(-1).repeat(1, 1, 3) {SPLIT[eij[0]].unsqueeze(-1).repeat(1, 1, 3).shape}\")\n",
    "#d_siu = scatter_sum(( SP * SPLIT[eij[0]] + SPLIT[eij[0]]), eij[0], dim=0)\n",
    "d_siu = SP * SPLITu2 + SPLITu3\n",
    "\n",
    "#print(f\"d_siu {d_siu.shape}\")\n",
    "vi += d_viu\n",
    "si += d_siu\n",
    "\n",
    "Lr = nn.Sequential(\n",
    "    Linear(in_features=128, out_features=64, bias=False),\n",
    "    SiLU(),\n",
    "    Linear(in_features=64, out_features=1, bias=False),\n",
    ")\n",
    "readout = Lr(si)\n",
    "print(readout)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import Linear, SiLU\n",
    "from torch_scatter import scatter_sum\n",
    "\n",
    "class Message(nn.Module):\n",
    "    def __init__(self, Ls=None, Lrbf=None, nRbf=20, nF=128):\n",
    "        super(Message, self).__init__()\n",
    "        self.Ls = Ls if Ls is not None else nn.Sequential(\n",
    "            Linear(nF, nF),\n",
    "            SiLU(),\n",
    "            Linear(nF, 3*nF),\n",
    "        )\n",
    "        self.Lrbf = Lrbf if Lrbf is not None else Linear(nRbf, 3*nF)\n",
    "\n",
    "    def fCut(self, rij_norm, rCut):\n",
    "        f_cut = 0.5 * (torch.cos(torch.pi * rij_norm / rCut) + 1)\n",
    "        f_cut[rij_norm > rCut] = 0 \n",
    "        return f_cut\n",
    "\n",
    "    def fRBF(self, rij_norm, rCut, nRbf=20):\n",
    "        Trbf = torch.arange(1, nRbf + 1, device=rij_norm.device).float()\n",
    "        rij_norm = rij_norm.unsqueeze(-1)  \n",
    "        RBF = torch.sin(Trbf * torch.pi * rij_norm / rCut) / (rij_norm + 1e-8)\n",
    "        return RBF\n",
    "\n",
    "    def forward(self, vj, sj, rij_vec, eij, rCut=5.0, nRbf=20):\n",
    "        rij_norm = torch.norm(rij_vec, dim=-1)\n",
    "        rij_hat =  rij_vec / (rij_norm.unsqueeze(-1) + 1e-8)\n",
    "\n",
    "        RBF = self.fRBF(rij_norm, rCut, nRbf)\n",
    "        T_RBF = self.Lrbf(RBF)\n",
    "        Ws = T_RBF * self.fCut(rij_norm,5.0).unsqueeze(-1) \n",
    "\n",
    "        phi = self.Ls(sj)\n",
    "        phiW = phi * Ws\n",
    "\n",
    "        SPLIT1 = phiW[:,0:128]\n",
    "        SPLIT2 = phiW[:,128:256]\n",
    "        SPLIT3 = phiW[:,256:]\n",
    "\n",
    "        phiWvv = vj * SPLIT1.unsqueeze(-1).repeat(1, 1, 3)\n",
    "        phiWvs = SPLIT3.unsqueeze(-1) * rij_hat.unsqueeze(1)\n",
    "        \n",
    "        d_vim = scatter_sum((phiWvv + phiWvs), eij[1], dim=0)\n",
    "        d_sim = scatter_sum(SPLIT2, eij[1], dim=0)\n",
    "        return d_vim, d_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Update(nn.Module):\n",
    "    def __init__(self, Luu=None, Luv=None, Ls=None):\n",
    "        super(Update, self).__init__()\n",
    "        self.Luu = Luu if Luu is not None else Linear(3, 3, False)\n",
    "        self.Luv = Luv if Luv is not None else Linear(3, 3, False)\n",
    "        \n",
    "        self.Ls = Ls if Ls is not None else nn.Sequential(\n",
    "            Linear(in_features=256, out_features=128),\n",
    "            SiLU(),\n",
    "            Linear(in_features=128, out_features=384),\n",
    "        )\n",
    "\n",
    "    def forward(self, vi, si):\n",
    "        Uvi = self.Luu(vi) \n",
    "        Vvi = self.Luv(vi)\n",
    "\n",
    "        V_norm = torch.norm(Vvi,dim=-1)\n",
    "        STACK = torch.hstack([V_norm, si])\n",
    "\n",
    "        SP = torch.sum(Uvi * Vvi, dim=-1) \n",
    "\n",
    "        SPLIT = self.Ls(STACK)\n",
    "        SPLIT1 = SPLIT[:, 0:128]\n",
    "        SPLIT2 = SPLIT[:, 128:256]\n",
    "        SPLIT3 = SPLIT[:, 256:]\n",
    "\n",
    "        d_viu = Uvi * SPLIT1.unsqueeze(-1).repeat(1, 1, 3)\n",
    "        d_siu = SP * SPLIT2 + SPLIT3\n",
    "\n",
    "        return d_viu, d_siu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import radius_graph\n",
    "\n",
    "class PaiNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Polarizable Atom Interaction Neural Network with PyTorch.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, Lm, Lu,\n",
    "        num_message_passing_layers: int = 3,\n",
    "        num_features: int = 128,\n",
    "        num_outputs: int = 1,\n",
    "        num_rbf_features: int = 20,\n",
    "        num_unique_atoms: int = 100,\n",
    "        cutoff_dist: float = 5.0,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_message_passing_layers: Number of message passing layers in\n",
    "                the PaiNN model.\n",
    "            num_features: Size of the node embeddings (scalar features) and\n",
    "                vector features.\n",
    "            num_outputs: Number of model outputs. In most cases 1.\n",
    "            num_rbf_features: Number of radial basis functions to represent\n",
    "                distances.\n",
    "            num_unique_atoms: Number of unique atoms in the data that we want\n",
    "                to learn embeddings for.\n",
    "            cutoff_dist: Euclidean distance threshold for determining whether \n",
    "                two nodes (atoms) are neighbours.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        #raise NotImplementedError\n",
    "        self.num_message_passing_layers = num_message_passing_layers\n",
    "        self.num_features = num_features\n",
    "        self.num_outputs = num_outputs\n",
    "        self.num_rbf_features = num_rbf_features\n",
    "        self.num_unique_atoms = num_unique_atoms\n",
    "        self.cutoff_dist = cutoff_dist\n",
    "\n",
    "        self.zi = nn.Embedding(num_unique_atoms, num_features)\n",
    "\n",
    "        self.Lm = Lm\n",
    "        self.Lu = Lu\n",
    "\n",
    "        self.Lr = nn.Sequential(\n",
    "            Linear(in_features=128, out_features=64),\n",
    "            SiLU(),\n",
    "            Linear(in_features=64, out_features=1),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        atoms: torch.LongTensor,\n",
    "        atom_positions: torch.FloatTensor,\n",
    "        graph_indexes: torch.LongTensor,\n",
    "    ) -> torch.FloatTensor:\n",
    "        si = self.zi(atoms)\n",
    "        eij = radius_graph(atom_positions, r=self.cutoff_dist, batch=graph_indexes)\n",
    "        sj = si[eij[0]]\n",
    "        vi = torch.zeros_like(si).unsqueeze(-1).repeat(1, 1, 3)\n",
    "        vj = vi[eij[0]]\n",
    "        rij_vec = atom_positions[eij[0]] - atom_positions[eij[1]]\n",
    "        for _ in range(self.num_message_passing_layers):\n",
    "            d_vim, d_sim = self.Lm(vj, sj, rij_vec, eij)\n",
    "            vi = vi + d_vim\n",
    "            si = si + d_sim\n",
    "\n",
    "            d_viu, d_siu = self.Lu(vi, si)\n",
    "\n",
    "            vi = vi + d_viu\n",
    "            si = si + d_siu\n",
    "        \n",
    "        Sigma = self.Lr(si)\n",
    "\n",
    "        return Sigma\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cli(args: list = []):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--seed', default=0)\n",
    "\n",
    "    # Data\n",
    "    parser.add_argument('--target', default=7, type=int) # 7 => Internal energy at 0K\n",
    "    parser.add_argument('--data_dir', default='data/', type=str)\n",
    "    parser.add_argument('--batch_size_train', default=100, type=int)\n",
    "    parser.add_argument('--batch_size_inference', default=1000, type=int)\n",
    "    parser.add_argument('--num_workers', default=0, type=int)\n",
    "    parser.add_argument('--splits', nargs=3, default=[110000, 10000, 10831], type=int) # [num_train, num_val, num_test]\n",
    "    parser.add_argument('--subset_size', default=None, type=int)\n",
    "\n",
    "    # Model\n",
    "    parser.add_argument('--num_message_passing_layers', default=3, type=int)\n",
    "    parser.add_argument('--num_features', default=128, type=int)\n",
    "    parser.add_argument('--num_outputs', default=1, type=int)\n",
    "    parser.add_argument('--num_rbf_features', default=20, type=int)\n",
    "    parser.add_argument('--num_unique_atoms', default=100, type=int)\n",
    "    parser.add_argument('--cutoff_dist', default=5.0, type=float)\n",
    "\n",
    "    # Training\n",
    "    parser.add_argument('--lr', default=5e-4, type=float)\n",
    "    parser.add_argument('--weight_decay', default=0.01, type=float)\n",
    "    parser.add_argument('--num_epochs', default=1000, type=int)\n",
    "\n",
    "    args = parser.parse_args(args=args)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/1000 [06:06<16:52:25, 61.11s/it, Train loss: 1.114e-02]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m     49\u001b[0m     loss_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m---> 50\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43matomic_contributions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpainn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m            \u001b[49m\u001b[43matoms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m            \u001b[49m\u001b[43matom_positions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgraph_indexes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\WenhaoLi\\DWINDOWS\\anaconda3\\envs\\painn\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\WenhaoLi\\DWINDOWS\\anaconda3\\envs\\painn\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\WenhaoLi\\DWINDOWS\\anaconda3\\envs\\painn\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\WenhaoLi\\DWINDOWS\\anaconda3\\envs\\painn\\Lib\\site-packages\\torch_geometric\\loader\\dataloader.py:27\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     25\u001b[0m elem \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, BaseData):\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_data_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_collate(batch)\n",
      "File \u001b[1;32mc:\\Users\\WenhaoLi\\DWINDOWS\\anaconda3\\envs\\painn\\Lib\\site-packages\\torch_geometric\\data\\batch.py:97\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[1;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_data_list\u001b[39m(\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     87\u001b[0m     exclude_keys: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     88\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m    list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m    Will exclude any keys given in :obj:`exclude_keys`.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m     batch, slice_dict, inc_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincrement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_num_graphs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_list)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_slice_dict \u001b[38;5;241m=\u001b[39m slice_dict  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\WenhaoLi\\DWINDOWS\\anaconda3\\envs\\painn\\Lib\\site-packages\\torch_geometric\\data\\collate.py:109\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# Collate attributes into a unified representation:\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m value, slices, incs \u001b[38;5;241m=\u001b[39m \u001b[43m_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# If parts of the data are already on GPU, make sure that auxiliary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# data like `batch` or `ptr` are also created on GPU:\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Tensor) \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mis_cuda:\n",
      "File \u001b[1;32mc:\\Users\\WenhaoLi\\DWINDOWS\\anaconda3\\envs\\painn\\Lib\\site-packages\\torch_geometric\\data\\collate.py:167\u001b[0m, in \u001b[0;36m_collate\u001b[1;34m(key, values, data_list, stores, increment)\u001b[0m\n\u001b[0;32m    165\u001b[0m     values \u001b[38;5;241m=\u001b[39m [value\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m values]\n\u001b[0;32m    166\u001b[0m sizes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([value\u001b[38;5;241m.\u001b[39msize(cat_dim \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m values])\n\u001b[1;32m--> 167\u001b[0m slices \u001b[38;5;241m=\u001b[39m \u001b[43mcumsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43msizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m increment:\n\u001b[0;32m    169\u001b[0m     incs \u001b[38;5;241m=\u001b[39m get_incs(key, values, data_list, stores)\n",
      "File \u001b[1;32mc:\\Users\\WenhaoLi\\DWINDOWS\\anaconda3\\envs\\painn\\Lib\\site-packages\\torch_geometric\\utils\\functions.py:23\u001b[0m, in \u001b[0;36mcumsum\u001b[1;34m(x, dim)\u001b[0m\n\u001b[0;32m     20\u001b[0m size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[:dim] \u001b[38;5;241m+\u001b[39m (x\u001b[38;5;241m.\u001b[39msize(dim) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, ) \u001b[38;5;241m+\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[dim \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m     21\u001b[0m out \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mnew_empty(size)\n\u001b[1;32m---> 23\u001b[0m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnarrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mzero_()\n\u001b[0;32m     24\u001b[0m torch\u001b[38;5;241m.\u001b[39mcumsum(x, dim\u001b[38;5;241m=\u001b[39mdim, out\u001b[38;5;241m=\u001b[39mout\u001b[38;5;241m.\u001b[39mnarrow(dim, \u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(dim)))\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = [] # Specify non-default arguments in this list\n",
    "args = cli(args)\n",
    "seed_everything(args.seed)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "dm = QM9DataModule(\n",
    "    target=args.target,\n",
    "    data_dir=args.data_dir,\n",
    "    batch_size_train=args.batch_size_train,\n",
    "    batch_size_inference=args.batch_size_inference,\n",
    "    num_workers=args.num_workers,\n",
    "    splits=args.splits,\n",
    "    seed=args.seed,\n",
    "    subset_size=args.subset_size,\n",
    ")\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "y_mean, y_std, atom_refs = dm.get_target_stats(\n",
    "    remove_atom_refs=True, divide_by_atoms=True\n",
    ")\n",
    "\n",
    "painn = PaiNN(\n",
    "    Lm=Message(),\n",
    "    Lu=Update(),\n",
    "    num_message_passing_layers=args.num_message_passing_layers,\n",
    "    num_features=args.num_features,\n",
    "    num_outputs=args.num_outputs, \n",
    "    num_rbf_features=args.num_rbf_features,\n",
    "    num_unique_atoms=args.num_unique_atoms,\n",
    "    cutoff_dist=args.cutoff_dist,\n",
    ")\n",
    "post_processing = AtomwisePostProcessing(\n",
    "    args.num_outputs, y_mean, y_std, atom_refs\n",
    ")\n",
    "\n",
    "painn.to(device)\n",
    "post_processing.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    painn.parameters(),\n",
    "    lr=args.lr,\n",
    "    weight_decay=args.weight_decay,\n",
    ")\n",
    "\n",
    "painn.train()\n",
    "pbar = trange(args.num_epochs)\n",
    "for epoch in pbar:\n",
    "\n",
    "    loss_epoch = 0.\n",
    "    for batch in dm.train_dataloader():\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        atomic_contributions = painn(\n",
    "            atoms=batch.z,\n",
    "            atom_positions=batch.pos,\n",
    "            graph_indexes=batch.batch\n",
    "        )\n",
    "        preds = post_processing(\n",
    "            atoms=batch.z,\n",
    "            graph_indexes=batch.batch,\n",
    "            atomic_contributions=atomic_contributions,\n",
    "        )\n",
    "        loss_step = F.mse_loss(preds, batch.y, reduction='sum')\n",
    "\n",
    "        loss = loss_step / len(batch.y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_epoch += loss_step.detach().item()\n",
    "    loss_epoch /= len(dm.data_train)\n",
    "    pbar.set_postfix_str(f'Train loss: {loss_epoch:.3e}')\n",
    "\n",
    "mae = 0\n",
    "painn.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in dm.test_dataloader():\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        atomic_contributions = painn(\n",
    "            atoms=batch.z,\n",
    "            atom_positions=batch.pos,\n",
    "            graph_indexes=batch.batch,\n",
    "        )\n",
    "        preds = post_processing(\n",
    "            atoms=batch.z,\n",
    "            graph_indexes=batch.batch,\n",
    "            atomic_contributions=atomic_contributions,\n",
    "        )\n",
    "        mae += F.l1_loss(preds, batch.y, reduction='sum')\n",
    "\n",
    "mae /= len(dm.data_test)\n",
    "unit_conversion = dm.unit_conversion[args.target]\n",
    "print(f'Test MAE: {unit_conversion(mae):.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "painn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
